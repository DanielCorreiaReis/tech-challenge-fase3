# ğŸ“š Fine-Tuning de LLM para GeraÃ§Ã£o e Resumo de Livros

Este notebook do **Google Colab** implementa o fine-tuning de um **Large Language Model (LLM)** utilizando **UnsLoTH** e **LoRA**, otimizando a geraÃ§Ã£o de descriÃ§Ãµes de livros.  

## âœ¨ Funcionalidades  
âœ… Monta o **Google Drive** para leitura e salvamento de dados  
âœ… Processa um dataset JSON contendo **tÃ­tulos e descriÃ§Ãµes de livros**  
âœ… Carrega e configura modelos **Llama-3** e outras variantes compatÃ­veis com **4-bit quantization**  
âœ… Gera **descriÃ§Ãµes detalhadas** para tÃ­tulos de livros  
âœ… Realiza **fine-tuning** do modelo com **SFTTrainer** para aprimorar a geraÃ§Ã£o de texto  
âœ… Testa o modelo treinado em novas entradas

## ğŸš€ Como Usar  
1ï¸âƒ£ **Configure** seu dataset no **Google Drive**  
2ï¸âƒ£ **Execute** as cÃ©lulas do notebook para **treinar o modelo**  
3ï¸âƒ£ **Teste** a geraÃ§Ã£o de descriÃ§Ãµes e resumos  
4ï¸âƒ£ **Salve** o modelo treinado para inferÃªncia futura  

## ğŸ“‚ Estrutura de Arquivos  
ğŸ“Œ `trn.json` â€“ Dataset inicial com **tÃ­tulos e descriÃ§Ãµes**  
ğŸ“Œ `amazon_titles_search.json` â€“ Dataset processado  
ğŸ“Œ `formatted_amazon_titles_search_data.json` â€“ Dados formatados para treinamento  
ğŸ“Œ `lora_model` â€“ **Modelo treinado salvo**  

## ğŸ›  Tecnologias Utilizadas  
ğŸ”¹ **Google Colab**  
ğŸ”¹ **UnsLoTH**  
ğŸ”¹ **LoRA**  
ğŸ”¹ **Transformers**  
ğŸ”¹ **Hugging Face Datasets**  
